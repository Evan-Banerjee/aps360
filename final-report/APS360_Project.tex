\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%######## APS360: Uncomment your submission name
%\newcommand{\apsname}{Project Proposal}
%\newcommand{\apsname}{Progress Report}
\newcommand{\apsname}{Final Report}

%######## APS360: Put your Group Number here
\newcommand{\gpnumber}{20}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{placeins}

%######## APS360: Put your project Title here
\title{Poetry Through Propogation,\ Generating Haikus With Deep Learning Recurrent Neural Networks}


%######## APS360: Put your names, student IDs and Emails here
\author{Evan Banerjee  \\
Student\# 1009682309\\
\texttt{evan.banerjee@mail.utoronto.ca} \\
\And
Diego Ciudad Real Escalante  \\
Student\# 1009345308 \\
\texttt{diego.ciudadrealescalante@mail.utoronto.ca} \\
\AND
Noah Monti  \\
Student\# 1009452398 \\
\texttt{noah.monti@mail.utoronto.ca} \\
\And
Ji Hong Sayo \\
Student\# 1007314728 \\
\texttt{ji.sayo@mail.utoronto.ca} \\
\AND
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
%######## APS360: Document starts here
\begin{document}


\maketitle

\begin{abstract}
  This is our final report for the APS360 Final Project.
  %######## APS360: Do not change the next line. This shows your Main body page count.
  Total Pages: \pageref{last_page}
\end{abstract}

\section{Introduction}

\begin{center}
  \textit{“Genuine poetry can communicate before it is understood” —T.S. Eliot}
\end{center}

In this project we are working to build a deep learning model to generate short poems of various topics.
Our model is a haiku generator which can be prompted with the first line of a haiku and generate the content of subsequent lines, built on a gated recurrent unit (GRU) architecture.
Poetry has long been regarded as a deep expression of emotion and human experience. Given that the model we construct will possess neither emotions nor intuition, we are interested to see whether our construction will be able to imitate human poetry, and if so to what degree. If the model is capable of producing compelling poems as output, this would contradict the idea that poetry requires emotion to produce, with implications for our understanding of the nature of creative work in general.

\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.85\textwidth]{Figs/pipeline.png}
  \end{center}
  \caption{Basic Model Pipeline}
  \label{fig:Basic Model Pipeline}
\end{figure}

Several ML approaches can be taken to text-generation. For instance, we compare our model to markov chains as a baseline. However, deep learning offers advantages that other statistical approaches cannot. As compared to simpler statistical models, neural nets can learn more abstract and general characteristics in the poems used for training. Given that a hallmark of good poetry is abstraction and unity around a sustaining theme, we believe deep learning is uniquely well-suited for the problem of poetry generation. 



\section{Illustriaton}
The high-level description of the model can be seen below in \ref{fig:model_diagram} .
The model consists of Gated Recurrent Unit that is trained on poems sourced from the web.
These poems also build up the dictionary of words used by the model to generate haikus.
Finally, the diagram also shows the expected usage of this model.
A user would prompt the model with the starting words of a poem, press enter,
and the model would generate a haiku with the prompt as its starting point.



\begin{figure}[h]
  \begin{center}
  \includegraphics[width=1\textwidth]{Figs/model_diagram.png}
  \end{center}
  \caption{Top-Level Model Description}
  \label{fig:model_diagram}
\end{figure}

\section{Background}
Previous poetry generation models utilized Recurrent Neural Networks (RNN) with some form of memory limiting to generate poems off a large dataset.\\\\
One model, created by researchers at the University of Saarland, utilized encoder decoder systems, as well as long short-term memory (LSTM) and Gated Recurrent Units (GRU) to address vanishing gradients, \citep{novikovaenglish}.\\\\
Another model, developed at the University of Edinburgh utilized multiple neural networks chained together, with a convolutional neural network (CNN) and multiple RNN for futher processing \citep{zhang2014chinese}.\\\\
Another model, built at City, University of London, used syllables as the inputs and outputs of their RNN instead of words \citep{lewis2021syllable}.\\\\
When focusing specifically on haikus, even a relatively basic RNN model built by students at Stanford University was able to score well \citep{haikugeneration}. Here, AI generated haikus were given a total score of 80\% of the human ones.\\\\
Finally, a model presented at the 13th conference on language resources and evaluation found a large degree of success in constraining the output of neural networks to generate poems \citep{popescu2022constrained}.
This model forced the AI to repeatedly output lines of a poem until certain syntax requirements were met, and swapped out words after generating lines to improve flow.\\\\
Each one of these model's utilized different techniques. Whether the differences were in the way they encoded and decoded the inputs and outputs, or in the specific method they used to manage hidden states, or in any other part of the model as a whole, each model had their own unique way of creating coherent output.\\\\ 
However, every model utilized some form of encoding and decoding the text, some RNN architecture with a hidden state to efficiently calculate gradients, some variant of softmax to normalize their output, and some probabilistic sampling to choose the next word. Many of the poems also implemented some algorithm to prevent individual lines of each poem from being incoherent. These principles are what guided our design as we developed our model throughout the semester, with our final work utilizing a GRU, individual words as tokens, grammar checks to constrain the output.\\\\



\section{Data Processing}
The main data used for this project consists of the Gutenberg Poetry Corpus dataset in
github \citep{gutenberg_dataset}.
This dataset is publicly available and comes from a web scraper run in 2018.
The web scraper collected poems from the Gutenberg Project, and online repository for public domain literature.
The data comes in the form of a JSON file which has the following fields:
"s", a line of poetry, and "gid" an id of the book in the gutemberg project database.
To clean the data we stripped every line of the poem, and segmented them into "poems" of 10 lines.
We decided to split the data every 10 lines since the main reason for using this data set is to draw from more historical and poetic languge.
Therefore, the actual length of the text we trained our model on is not as relevant since we work out the structure of the haiku through the training.

Another data source we used is the CMU pronouncing dictionary \citep{cmu_pronouncing_dictionary}.
We use this dictionary to count the amount of syllables in each line of the haiku.
Given that this is an open source project with a python library, data cleanup was minimal for this dataset.


After downloading the data, the cleaning process for the haiku dataset consists is run by a python script.
This python script takes in each line of the JSON file, and removes uncommon characters such as the return-carriage character (\texttt{\textbackslash r}), tabs (\texttt{\textbackslash t}), and em-dashes (—).
Next it converts all the characters to lowercase.
And finally, it appends the haiku to a text file in which the end of a poem section is denoted by two new line characters.
The text file is then used at training when loading the data.
Below is an example of the raw input data, and its processed form.



\begin{multicols}{2}

  \textbf{Original (Raw text):} \par
\{ "s": "That was our bench the time you said to me", "gid": "442" \} \\
\{ "s": "The long new poem--but how different now,", "gid": "442" \} \\
\{ "s": "How eerie with the curtain of the fog", "gid": "442" \} \\
\{ "s": "Making it strange to all the friendly trees!", "gid": "442" \} \\
\{ "s": "There is no wind, and yet great curving scrolls", "gid": "442" \} \\
\{ "s": "Carve themselves, ever changing, in the mist.", "gid": "442" \} \\
\{ "s": "Walk on a little, let me stand here watching", "gid": "442" \} \\
\{ "s": "To see you, too, grown strange to me and far. . . .", "gid": "442" \} \\
\{ "s": "I used to wonder how the park would be", "gid": "442" \} \\
\{ "s": "If one night we could have it all alone--", "gid": "442" \} \\

  \columnbreak

  \raggedleft
  \textbf{Finalized (After cleaning):} \par

  \raggedleft
That was our bench the time you said to me \\
The long new poem but how different now \\
How eerie with the curtain of the fog \\
Making it strange to all the friendly trees \\
There is no wind, and yet great curving scrolls \\
Carve themselves, ever changing, in the mist \\
Walk on a little, let me stand here watching \\
To see you, too, grown strange to me and far \\
I used to wonder how the park would be \\
If one night we could have it all alone


\end{multicols}

Another use of the data is in building the vocabulary before training.
To do this we run the entirety of the text file word-by-word into a python dictionary that maps each word to an index.
This lets us represent the words encountered in a numeric form that is easier to deal with.
Finally, we must say that because the nature of this project is generative,
testing new data simply means prompting the model differently.
In adition to being the largest collection of poems we could find,
this data set provides the additional benefit of having a wide range of styles.
This allows us to make more diverse poems as seen below in the qualitative results section.

\section{Architecture}

For our final model, we switched to a neural network centred around the gated recurrent unit (GRU) architecture, which replaced our previous long-short term memory (LSTM) architecture. 
The gated recurrent unit has been shown to be similar to the LSTM architecture on performance and quality of output, while requiring less compute resources during training \citep{LSTMvGRU}, which
appeared to hold true when testing our model and comparing to our previous design. 

%Figure~\ref{fig:flowchart} flowchart.

\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.95\textwidth]{Figs/Haiku-Generation.png}
  \end{center}
  \caption{Model Architecture}
  \label{fig:flowchart}
\end{figure}

The architecture of the model is as follows, as described in the Figure~\ref{fig:flowchart} flowchart.

Firstly, the model parameters are manually configured. These include the following, with the hyperparameters we found to work best with our dataset:

\begin{enumerate}
  \item The size of the vocabulary dictionary - which was 109976 distinct words for our data
  \item Embedding dimensions - 128 was found to work well
  \item The index of the padding token within the vocabulary dictionary
  \item The number of hidden dimensions - set to 256
  \item The number of GRU layers - set to 3
  \item How much dropout to apply between layers - we did not use any
  \item If the GRU will perform bidirectional passes - we did not perform these passes
\end{enumerate}

When data is passed to the model, it first goes though the embedding layer. This layer serves to transform this data into vector representations that capture the relationships between words throughout the data. 
The layer takes in the number of unique words in the dataset, the size of the embedding dimension, and the padding token index so that it does not contribute to gradients when training.

The output of that layer is then passed to the Gated Recurrent Unit layer, which serves to
process the sequence of embeddings to capture contextual information across words. It takes in the embedding dimensions, the hidden dimensions, 
the number of GRU layers to have, batch\_first (which should always be set to true for our data),
how much dropout to apply, and if the GRU will perform bidirectional passes. The resulting output of this layer are two tensors, 
one with the output features from the GRU, and another that has the hidden states of the GRU for each layer.

Finally, the output features are passed to a linear layer, which maps them as logits to each word in the vocabulary set. The input size to the layer is the number of input features, 
which is the size of the hidden dimension, and the output size is the number of output features, equal to the size of the vocabulary set.

It should be noted that if the GRU is to perform bidirectional passes, the number of hidden dimensions must be multiplied by two in order to align with the required tensor size.

This model can then be passed to train, for which we used Cross Entropy Loss criterion with the padding index passed as a parameter to ensure it does not contribute to gradients, 
and the Adaptive Movement Estimation (Adam) optimizer. Additionally, we used a learning rate of 0.001, a batch size of 64, and 50 epochs.

\section{Baseline Model}

Our first main baseline model is a markov chain of depth four. This was trained on the same data used in the primary model.
The main structure is shown in figure 0:

\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.75\textwidth]{Figs/MARKOV.png}
  \end{center}
  \caption{general weighting scheme for markov chain}
  \label{fig:plot}
  \end{figure}

Each individual word in the corpora is assigned a weighting scheme for every word (including itself) based on how frequently those other words come
after the first word.\\
Then every pair of words is assigned a weighting scheme for every word in the corpora based on how likely the word appears after the pair.\\
Then every triple of words is assigned a similar weighting scheme.\\
This repeats until you have a weighting scheme for every n-gram sequence of words up to the depth provided by the function.\\

Throughout this process we treated newline characters and "end of poem" lines (EOP) as distinct words. The model would take in a starting string of
words and would try to autocomplete the poem. It chose words by taking a weighted random choiced based on the weights calculated for the markov
datastructure\\

We chose to use a depth of 4 because lower depths had far more incoherent outputs while higher depths seemed to either return no output
or just directly copy poems for the most part. Here are some example inputs, and their corresponding outputs:\\


Show me:
\begin{flushleft}
  \textit{
      \hspace{2em} show me a garden that's \\
      \hspace{2em} bursting into life
  }
\end{flushleft}

Give:
\begin{flushleft}
  \textit{
      \hspace{2em} give your \\
      \hspace{2em} puppies a little extra\\
      \hspace{2em} thankful that god took me out\\
      \hspace{2em} this funny as hell\\
      \hspace{2em} steve's hair evolving\\
  }
\end{flushleft}


 Teach:
 \begin{flushleft}
  \textit{
      \hspace{2em} teach people  \\
      \hspace{2em} how to get folder icons\\
      \hspace{2em} to show in snowboard\\
  }
\end{flushleft}

It should be noted that these poems were hand-chosen from the markov chain for being more coherent. Often, the markov chain will simply copy
off another poem, return no output, or only return a couple of words, such as the input "teach" generating the output "teach great place to live".\\

Even in the best case, this model struggles to correctly write lines with the correct number of syllables.

We created another baseline model from a very simple LSTM RNN. the structure for this one was a much simpler version of our primary model.\\

It had a 128 dimensional embedding scheme with 2 hidden layers of dimension 256 and a learning rate of 0.001. It was trained with a batch size of 64
for 30 epochs. The output was mostly incoherent so we decided to use the Markov chain as a baseline. All the source code for the first RNN and the
Markov chain is on github, under the jupyter notebook \texttt{RNN\_Model\_1}.


\section{Quantitative Results}

We tested out model with 300 one or two word prompts to the model, and counted the syllables in the output haiku.

\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.8\textwidth]{Figs/syllable_count_last_word-1.png}
  \end{center}
  \caption{Syllables per haiku from 300 outputs}
  \label{fig:plot1}
\end{figure}

\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.8\textwidth]{Figs/loss-vs-epochs-2.png}
  \end{center}
  \caption{Training Loss over 50 Epochs}
  \label{fig:lossvepochs}
\end{figure}

As shown in Figure~\ref{fig:plot1}, all haikus that were generated by the model meet the 17 syllable count, 
demonstrating the model’s ability to regenerate words dynamically in order to maintain the traditional haiku structure - which was one of the targets from our previous model iteration.

During training, the loss and the rate of loss steadily decreases over the epochs as seen in Figure~\ref{fig:lossvepochs}, showing our model is continuing to learn the meanings of words relative to their neigbours in sentences. 
The loss is calculated as the rate of accuracy between the model's prediction of the next word in a sequence and the original word in that sequence, which reached 4.5 after 50 epochs.

\section{Qualitative Results}
The model generates a mix of interesting or funny hakius, and incoherent outputs:


\begin{multicols}{2}


  \textit{"river as thou fledst \\
  she glows within thy dreams where \\
  babe sad favorite"}

  \textit{"water from the breeze \\
  there love dejected lies not \\
  here and could he dream"}

  \textit{"grass and the moon is \\
  the smoke of a broken tree \\
  is great for the sun"}

  \textit{"ocean says she has \\
  to stir stainless nightingale \\
  but well they judge me”}


  \columnbreak

  \textit{“car wolves reach of sighs \\
  and coat an hurrah like strife \\
  truest mother round”}

  \textit{“long does fairer by \\
  their heart yet cracks and half so \\
  late they bent him off”}

  \textit{“when the death is no \\
  knight ere for my dower and the \\
  vine and hints of that”}

  \textit{"wake when spite of love \\
  when that their light are red and \\
  many a star sweet”}


\end{multicols}

As we can see, the model is able to generate haikus that could pass for human-written, however, it still generates
incoherent haikus.
While the decision to make train the haikus in poetry rather than haikus resulted in more poetic language,
it also increased the frequency of rarely used words.

The main challenges we faced making this model were
related to ensuring syllable structure and was maintained and the syllable counts were accurate, and
slow training times.

Because of irregularities in the English Language, we found it difficult to
come up with a reliable method to count the number of syllables in a given word. Our current solution involves referencing
the CMU Pronouncing Dictionary \citep{cmu_pronouncing_dictionary} to get the syllable count of a given word.
However, this method is not perfect, as the CMU Pronouncing Dictionary does not contain all words that may show up in our vocabulary.
This causes issues when a word is not in the dictionary, so we opted for heuristic algorithms to fix this,
although this is still not perfect in practice, it significantly reduced the number of times the 5-7-5 meter was broken.

Despite gaining more resources for training through Vertex AI,
this is not a sustainable way of training the model as it quickly runs out of free compute credits.
However, using even just the free Tier drastically increased the speed at which we were able to train the model.

\section{Evaluation on new data}
To evaluate our model, we generated dozens of new poems from varying inputs and chose the five that we believed were the most coherent to represent the model. We then created a survey with 15 poems. These consisted of the 5 made by our model, 5 chosen from the more coherent outputs of the baseline markov chain, and 5 chosen from the repository of human written poems that we used in training. We had 24 people rate the quality of each poem from 1 to 10, and averaged all the responses. The results are shown in the figure below:

\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.8\textwidth]{Figs/survey_results.png}
  \end{center}
  \caption{Syllables per haiku from 300 outputs}
  \label{fig:plot2}
\end{figure}
\FloatBarrier

the five poems used in the survey from our model are:

\begin{itemize}


  \item \textit{"winter chill with hopes \\
  of peace they see the home to \\
  sleep and we are heard"}

  \item \textit{"new grass like diamonds \\
  blowing through the darker sounds \\
  of heaven to flame"}

  \item \textit{"the mightiest makes \\
  the memory of some three \\
  shepherds from the glow"}

  \item \textit{"the charm of his heart \\
  the graceful hills of the brow \\
  and the black essence}

  \item \textit{the ravenous sand \\
  the graveyard for the raindrops \\
  and the gothic feet}

\end{itemize}

\section{Ethical Considerations}

Despite AI being around for several decades, it was only recently that computational power necessary to generate human-level works of art became widely available.
As impressive as they are, Increasingly improving models that generate images, written stories, and poems also raise questions about how these models get and utilize their training data.
While this is a very trending topic, legislation protecting individuals from having their written works used to train AI models is still in early stages.
Because of this we have decided to only use works in the public domain.
The works we use in this model can all be found in the Gutemberg Project digital library meaning none of them are held under exclusive rights.
Finally, this model is a purely academic experiment, and we do not seek to benefit from the material it generates.
Therefore, the literary works we will deal with in this project and our use of them fall under fair dealings as per Canadian law \citep{LegislativeServicesBranch_2024} .


%\pagebreak

\newpage
\clearpage

\label{last_page}

\bibliography{APS360_ref}
\bibliographystyle{iclr2022_conference}

\end{document}
